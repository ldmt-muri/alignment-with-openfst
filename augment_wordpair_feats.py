import re
import time
import io
import sys
import argparse
from collections import defaultdict

# parse/validate arguments
argParser = argparse.ArgumentParser()
argParser.add_argument("-sb", "--src_brown_filename")
argParser.add_argument("-tb", "--tgt_brown_filename")
argParser.add_argument("-i", "--input_filename") # this is the word alignment wordpair features generated by cdec
argParser.add_argument("-o", "--output_filename")
argParser.add_argument("-mspf", "--min_src_prefix_freq", type=int, default=4, help="for a src prefix to be extracted as a feature, it must appear in at least as many word types as this number")
argParser.add_argument("-mtpf", "--min_tgt_prefix_freq", type=int, default=4, help="for a tgt prefix to be extracted as a feature, it must appear in at least as many word types as this number")
argParser.add_argument("-spl", "--src_prefix_length", type=int, default=6, help="length of src prefixes to be potentially considered as features") 
argParser.add_argument("-tpl", "--tgt_prefix_length", type=int, default=6, help="length of tgt prefixes to be potentially considered as features") 
args = argParser.parse_args()

input_wordpair_file = io.open(args.input_filename, encoding='utf8', mode='r')
output_wordpair_file = io.open(args.output_filename, encoding='utf8', mode='w')

# TODO: this method needs a better name
def read_brown_clusters(brown_filename, prefix_length):
  brown_file = io.open(brown_filename, encoding='utf8', mode='r')
  # word type -> brown cluster, and word type -> token frequency
  type_to_cluster, type_to_freq = {}, {}
  # prefix -> type frequency
  prefix_to_freq = defaultdict(int)
  # just a list of frequencies to compute quantiles
  freqs = []
  # read the brown clustering "path" file
  for line in brown_file:
    if len(line.strip()) == 0: continue
    if line.startswith('#'): continue
    bitstring, word_type, freq = line.strip().split('\t')
    type_to_cluster[word_type] = bitstring
    # token frequency
    freq = int(freq)
    type_to_freq[word_type] = freq
    # frequency here refers to type frequencies, the number of unique word types in which a prefix appears
    if prefix_length > 0: prefix_to_freq[ word_type[0:prefix_length] ] += 1
    freqs.append(freq)
  brown_file.close()
  freqs.sort()
  return type_to_cluster, type_to_freq, freqs, prefix_to_freq

# freq_score \in \{0 \ldots max_score}
def compute_freq_score(word, word_freqs, sorted_freqs, max_score=4):
  fold_length = len(sorted_freqs) / (max_score-1);
  # special case: return score  = 1 if the word is out of vocab (with respect to word_freqs)
  if word not in word_freqs: return 0
  # special case: return score = max_score for the most frequent 100 words
  if word_freqs[word] > sorted_freqs[-100]: return max_score
  for fold in range(1, max_score):
    if word_freqs[word] < sorted_freqs[fold * fold_length]:
      return fold
  # this should only happen in peculiar cases (e.g. max_score > 100)
  return max_score

# read src/tgt clusters
src_brown_clusters, src_word_freqs, src_sorted_freqs, src_prefix_to_freqs = read_brown_clusters(args.src_brown_filename, args.src_prefix_length)
tgt_brown_clusters, tgt_word_freqs, tgt_sorted_freqs, tgt_prefix_to_freqs = read_brown_clusters(args.tgt_brown_filename, args.tgt_prefix_length)

# read/write word pairs
for line in input_wordpair_file:
  if len(line.strip()) == 0: continue
  src_word, tgt_word, features = line.strip().split(' ||| ')
  # first, determine the full brown cluster bitstring of src_word and tgt_word
  src_cluster = src_brown_clusters[src_word] if src_word in src_brown_clusters else u'?'
  tgt_cluster = tgt_brown_clusters[tgt_word] if tgt_word in tgt_brown_clusters else u'?'
  # then, determine how frequent src_word and tgt_word are
  max_score = 10
  #src_freq_score = compute_freq_score(src_word, src_word_freqs, src_sorted_freqs, max_score)
  #tgt_freq_score = compute_freq_score(tgt_word, tgt_word_freqs, tgt_sorted_freqs, max_score)

  # generate a bunch of binary features
  new_features = set()

  # null is a special case
  if src_word == u'<eps>':
    src_word = u'__null__token__'
  elif tgt_word == u'<eps>':
    tgt_word = u'__null__token__'
  else:
    # the full src/tgt clusters
    #new_features.add(u'Brown{}:{}=1'.format(src_cluster, tgt_cluster))
    # prefixes of the src/tgt clusters
    if len(src_cluster) < 4: src_cluster = u'{}{}'.format(src_cluster, u'xxx') 
    if len(tgt_cluster) < 4: tgt_cluster = u'{}{}'.format(tgt_cluster, u'xxx') 
    new_features.add(u'Brown:{}:{}=1'.format(src_cluster[0:4], tgt_cluster[0:4]))
    #new_features.add(u'Brown:{}:{}=1'.format(src_cluster[0:8], tgt_cluster[0:8]))
    #new_features.add(u'Brown:{}:{}=1'.format(src_cluster[0:12], tgt_cluster[0:12]))
    # frequency for src word and tgt word
    #new_features.add(u'Freq{}:{}=1'.format(src_freq_score, tgt_freq_score))
    #freq_score_diff = abs(src_freq_score - tgt_freq_score)
    #if src_freq_score == 0 or src_freq_score == max_score or \
    #   tgt_freq_score == 0 or tgt_freq_score == max_score:
    #  pass
    #elif freq_score_diff <= 1:
    #  pass
    #  new_features.add(u'FreqDiffSmall=1')
    #elif freq_score_diff <= max_score / 2.0:
    #  pass
    #  new_features.add(u'FreqDiffMedium=1')
    #else:
    #  pass
    #  new_features.add(u'FreqDiffLarge=1')
    # determine the src_prefix:tgt_prefix feature
    if args.src_prefix_length > -1 and args.tgt_prefix_length > -1:
      src_prefix, tgt_prefix = src_word[0:args.src_prefix_length], tgt_word[0:args.tgt_prefix_length]
      if src_prefix in src_prefix_to_freqs and \
            src_prefix_to_freqs[src_prefix] >= args.min_src_prefix_freq and \
            tgt_prefix in tgt_prefix_to_freqs and \
            tgt_prefix_to_freqs[tgt_prefix] >= args.min_tgt_prefix_freq:
        new_features.add(u'Prefix:{}:{}=1'.format(src_prefix, tgt_prefix))
        
  # write the new features to the end of the line
  line = u'{} ||| {} ||| {} {}\n'.format( src_word, tgt_word, features, u' '.join(new_features) )
  output_wordpair_file.write(line)
    
input_wordpair_file.close()
output_wordpair_file.close()
